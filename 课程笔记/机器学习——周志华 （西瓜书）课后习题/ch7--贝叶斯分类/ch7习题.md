#### 7.1 试使用极大似然法估算回瓜数据集 3.0 中前 3 个属性的类条件概率.
**答：**   

以第一个属性色泽为例，其值计数如下：

|好瓜\色泽 | 乌黑 |浅白 |青绿|
|---|---|---|---|
|否|2|4|3|
|是 |4 |1 |3|

令 $ p_{乌黑|是}=p_{1} $ 表示好瓜中色泽为“乌黑”的概率，$ p_{浅白|是}=p_{2} $ 为好瓜中“浅白”的概率， 
$ p_{青绿|是}=p_{3} $ ， $ p_{3} = 1-p_{2}-p_{3} $ ， $ D_{是} $ 表示好瓜的样本，其他类同，
于是色泽属性的似然概率则可表示为$ L(p)=P(X_{色泽}|Y=是)=\prod_{x\in D_{是}}P(x)=p_{1}^{4}p_{2}^{1}(1-p_1-p_{2})^{3} $ ,
其对数似然为：$ LL(p)=4ln(p_1)+ln(p_2)+3ln(1-p_1-p_2) $ ，分别对 $ p_1,p_2 $ 求偏导并使其为零，
即可得 $ p_1,p_2 $ 的极大似然估计： $ \hat{p_1}=\frac{1}{2},\hat{p_2}=\frac{1}{8} $,
$ \hat p_{3} =1-\hat p_{2}-\hat p_{3}=\frac{3}{8} $ ，同理可得  $ P(X_{色泽}|Y=否) $ 的似然概率，
进而求得类为“否”时各取值条件概率的极大似然估计。

其他两个属性同理。

---

#### 7.2* 试证明:条件独立性假设不成立时，朴素贝叶斯分类器仍有可能产生最优贝叶斯分类器.
**答：**   

令一组数据中有 $ A,B,C $ 三中属性，其目标值为 $ Bool $ 型变量，即取值 $ +,- $ 且其概率相同，即 $ P(+)=P(-)=\frac{1}{2} $ 。
给定一个样本 $ E $ ，令 $ P(A|+) $ 表示 $ P(A=a_{E}|+) $ ， $ a_E $ 表示 $ E $ 中属性 $ A $ 的取值。假设 $ A,C $ 完全独立，
而 $ A=B $ 即两者完全相关，因此理论上对于最优贝叶斯分类器来说，属性 $ B $ 应该被忽略，那么基于最优贝叶斯分类器时，
其决策准则（书中 P7.15 式）可描述为：若 $ P(+)P(A|+)P(C|+)-P(-)P(A|-)P(C|-)=P(A|+)P(C|+)-P(A|-)P(C|-)>0 $ 时，
则将样本 $ E $ 归为 $ + $ 类；而另一方面，考虑朴素贝叶斯分类器，在分类决策时会将属性 $ B $ 也考虑在内，
此时相当于将 $ A $ 计算了两次，此时决策准则则描述为： $ P(A|+)^2P(C|+)-P(A|-)^2P(C|-)>0 $ 时，将 $ E $ 归为 $ + $ 类。

根据贝叶斯理论， $ P(A|+) = P(A)P(+|A)/P(+) $ ，且由于 $ P(+)=P(-) $ ，则最优贝叶斯分类可表示为： $ P(+|A)P(+|C)-P(-|A)P(-|C)>0 $ ；
而朴素贝叶斯则表示为： $ P(+|A)^2P(+|C)-P(-|A)^2P(-|C)>0 $ 。取 $ P(+|A)=p,P(+|C)=q $ ，
于是最优贝叶斯分类器为： $ pq-(1-p)(1-q)=p+q-1>0\Rightarrow q>1-p $ ，
朴素贝叶斯为 $ p^2q-(1-p)^2(1-q)>0\Rightarrow q>\frac{(1-p)^2}{p^2+(1-p)^2} $ ，两者决策边界如下图：   
![1](https://github.com/han1057578619/MachineLearning_Zhouzhihua_ProblemSets/blob/master/ch7--%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB/image/1.jpg)

只有在两者决策边界之间（浅黄色区域），其分类情况是不同的，在其他区域，朴素贝叶斯分类结果和最优贝叶斯的分类结果是相同的，
因此即便属性之间独立性假设不成立，朴素贝叶斯在某些条件（本例中就是属性概率分布在两者相交区域之外）下任然是最优贝叶斯分类器。

参考：
- 《On the Optimality of the Simple Bayesian Classifier under Zero-One Loss》

ps.这个例子就是来自该论文，只做了一点翻译工作。论文中给出了更全面的理论证明，和朴素贝叶斯产生最优贝叶斯分类的充分必要条件。
本打算看完把理论证明也尝试复述一遍，但能力有限，一方面没有理解很透彻，另一方面证明过程有点长感觉表达能力有点不大够用...

---

#### 7.3 试编程实现拉普拉斯修正的朴素贝叶斯分类器，并以西瓜数据集 3.0 为训练集，对 p.151 "测1" 样本进行判别.
**答：** 

代码在：[7.3](https://github.com/han1057578619/MachineLearning_Zhouzhihua_ProblemSets/tree/master/ch7--%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB/7.3)

---

#### 7.4 实践中使用式 (7.15)决定分类类别时，若数据的维数非常高，则概率连乘 $ \prod_{i=1}^{d}P(x_i|c) $ 的结果通常会非常接近于 0 从试述防止下溢的可能方案.而导致下溢.
**答：** 

这在p153中已经给出答案了。即取对数将连乘转化为“连加”防止下溢。

即将式（7.15）改为：$ h_{nb}(x)=\begin{equation} 	\mathop{\arg\max}_{\theta}log(P(c))+\sum\_{i=1}^{d}log(P(x_i|c)) \end{equation} $ 。

---

#### 7.5 试证明:二分类任务中两类数据满足高斯分布且方差相同时，线性判别分析产生贝叶斯最优分类器.
**答：** 

首先看一下贝叶斯最优分类器：在书中p148中解释了对于最小化分类错误率的贝叶斯最优分类器可表示为： 
$ h^{\ast}(x)=\begin{equation} 	\mathop{\arg\max}\_{c\in y}P(c|x) \end{equation} $，
由贝叶斯定理即转换为： $ h^{\ast}(x)=\begin{equation} 	\mathop{\arg\max}\_{c\in y}P(x|c)P(c) \end{equation} $ 。
那么在数据满足高斯分布时有： 

$ h^*(x)=\begin{equation} 	\mathop{\arg\max}_{c\in y}P(x|c)P(c) \end{equation} $
$ =\begin{equation} 	\mathop{\arg\max}\_{c\in y}log(f(x|c)P(c) )\end{equation} $
$ =\begin{equation} 	\mathop{\arg\max}\_{c\in y}log(\frac{1}{(2\pi)^{n/2}\left| \Sigma \right|^{1/2}}exp(-\frac{1}{2}(x-\mu_c)^T \Sigma^{-1}(x-\mu_c))) + log(P(c))\end{equation} $ 
$ =\begin{equation} 	\mathop{\arg\max}\_{c\in y} -\frac{1}{2}(x-\mu_c)^T \Sigma^{-1}(x-\mu_c) + log(P(c)) \end{equation} $ 
$ =\begin{equation} 	\mathop{\arg\max}\_{c\in y} x^T\Sigma^{-1}\mu_c - \frac{1}{2}\mu_c^T\Sigma^{-1}\mu_c + log(P(c)) \end{equation} $ 

在二分类任务中，贝叶斯决策边界可表示为    
$ g(x)= x^T\Sigma^{-1}\mu_1 -x^T\Sigma^{-1}\mu_0 -  (\frac{1}{2}\mu_1^T\Sigma^{-1}\mu_1 - \frac{1}{2}\mu_0^T\Sigma^{-1}\mu_0) +  log(\frac{P(1)}{P(0)}) $
$ =x^T\Sigma^{-1}(\mu_1-\mu_0)- \frac{1}{2}(\mu_1+\mu_0)^T\Sigma^{-1}(\mu_1-\mu_0) +\log(\frac{P(1)}{P(0)}) $


再看看线性判别分析：

书中 p62 给出式 3.39 ，其投影界面可等效于 $ w=(\Sigma_0+\Sigma_1)^{-1}(\mu_1-\mu_0) $ ，注意为了和上面的推导一致，
这里和书中给出的差了一个负号，但 $ w $ 位置没有改变，只是改变了方向而已。在两类别方差相同时有： $ w=\frac{1}{2}\Sigma^{-1}(\mu_1-\mu_0) $ ，
两类别在投影面连线的中点可为 $ \frac{1}{2}(\mu_1+\mu_0)^Tw= \frac{1}{4}(\mu_1+\mu_0)^T\Sigma^{-1}(\mu_1-\mu_0) $ ，
那么线性判别分析的决策边界可表示为 $ g(x)=x^T\Sigma^{-1}(\mu_1-\mu_0)-\frac{1}{2}(\mu_1+\mu_0)^T\Sigma^{-1}(\mu_1-\mu_0) $ 。

推导到这里发现贝叶斯最优分类器和线性判别分析的决策边界只相差 $ log(\frac{P(1)}{P(0)}) $ ，
不知道这里是不是因为我推导错了、中间漏了什么原因。若再加上两类别概率也相同，那么就完全符合题意了。暂且先这么答吧。

---

#### 7.6 试编程实现 AODE 分类器，并以西瓜数据集 3.0 为训练集，对 p.151的"测1" 样本进行判别.
**答：** 

代码在[7.6-AODE](https://github.com/han1057578619/MachineLearning_Zhouzhihua_ProblemSets/tree/master/ch7--%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB/7.6)

提一下关于连续值处理的问题。这个书中和原论文(好像)都没有提交，所以按照自己的理解来处理了。考虑到以下，实现过程中不将连续值作为父属性。

- 此外AODE本身就是要将取值样本数量低于一定阈值(论文中给出的是30)的属性去除的，从这个角度来说，连续值就不能作为父属性了，当前其实可以按照区间划分将连续值离散化。

另外，虽然在样本这么小的情况下，看预测结果实际意义不大，但相比于朴素贝叶斯，AODE对于西瓜数据集的拟合更好（错误率更低）。
​    
ps.书中给出的式(7.24)有错误的，分母的 $ N_i $ 改正为 $ N\times N_i $ ，在第十次印刷的时候纠正了，看旧版书的同学要注意了。

---

#### 7.7 给定 d 个二值属性的二分类任务，假设对于任何先验概率项的估算至少需 30 个样例，则在朴素贝叶斯分类器式 (7.15) 中估算先验概率项 $ P(c) $ 需 30 x 2 = 60 个样例.试估计在 AODE 式 (7.23) 中估算先验概率项 $ P(c,x_i) $ 所需的样例数(分别考虑最好和最坏情形) .
**答：** 

这里“假设对于任何先验概率项的估算至少需 30 个样例”意味着在所有样本中， 任意 $ c,x_i $ 的组合至少出现30次。

当 $ d=1 $ 时，即只有一个特征 $ x_1 $ ，因为是二值属性，假设取值为 $ 1,0 $ ，那为了估计 $ p(y=1,x_1=1) $ 至少需要 30 个样本，
同样 $ p(y=1,x_1=0) $ 需要额外 30 个样本，另外两种情况同理，所以在 $ d=1 $ 时，最好和最坏情况都需要 120 个样本。

再考虑 $ d=2 $ ，多加个特征 $ x_2 $ 同样取值 $ 1,0 $ ，为了满足求 $ P(c,x_1) $ 已经有了 120 个样本，且 60 个正样本和 60 个负样本；
在最好的情况下，在 60 个正样本中，正好有 30 个样本 $ x_2=1 $ , 30 个 $ x_2=0 $ ，负样本同理，此时这 120 个样本也同样满足计算 $ P(c,x_2) $ 的条件，
所有 $ d=2 $ 时，最好的情况也只需要 120 个样本，$ d=n $ 时同理；在最坏的情况下，120 个样子中， $ x_2 $ 都取相同的值 1 ，
那么为了估算 $ P(c,x_2=0) $ 需要额外 60 个样本，总计 180 个样本，同理计算出 $ d=2,3,4... $ 时的样本数，即每多一个特征，
最坏情况需要多加额外 60 个样本， $ d=n $ 时，需要 $ 60(n+1) $ 个样本。

那么 $ d $个二值属性下，最好情况需要 120 个样本，最好情况需要 $ 60(d+1) $ 个样本。

---

![2](https://github.com/han1057578619/MachineLearning_Zhouzhihua_ProblemSets/blob/master/ch7--%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB/image/2.jpg)   
**答：** 

**这个问题主要基于书中式7.26，就很容易理解了。**

首先考虑同父结构，根据式7.26，其联合分布可以表示为：$ P(x_1,x_3,x_4)=P(x_1)P(x_3|x_1)P(x_4|x_1) $    

在给定 $ x_1 $ 时， $ P(x_3,x_4|x_1)=\frac{P(x_1,x_3,x_4)}{P(x_1)}=\frac{P(x_1)P(x_3|x_1)P(x_4|x_1)}{P(x_1)}=P(x_3|x_1)P(x_4|x_1) $

即同父结构中 $ x_3,x_4 $ 关于 $ x_1 $ 条件独立；

在 $ x_1 $ 取值未知时有：$ P(x_3,x_4)=\sum_{x_1}P(x_1,x_3,x_4)=\sum_{x_1}P(x_1)P(x_3|x_1)P(x_4|x_1) $ 是推不出 $ P(x_3,x_4)=P(x_3)P(x_4) $ 的，

所以同父结构中 $ x_3,x_4 $ 关于 $ x_1 $ 边际独立不成立。

再考虑顺序结构，其联合分布有： $ P(x,y,z)=P(z)P(x|z)P(y|x) $

给定 $ x $ 时，

$ P(y,z|x)=\frac{P(x,y,z)}{P(x)}=\frac{P(z)P(x|z)P(y|x)}{P(x)}=\frac{P(x,z)P(y|x)}{P(x)}=P(z|x)P(y|x) $ 

，即顺序结构中 $ y,z $ 关于 $ x $ 条件独立；

在 $ x $ 取值未知时有：

$ P(y,z)=\sum_xP(x, y, z)=\sum_xP(z)P(x|z)P(y|x) $ ，同样推不出 $ P(y,z)=P(y)P(z) $ ，所以顺序结构中，同样 $ y,z $ 关于 $ x $ 边际独立不成立。

---

#### 7.9 以西瓜数据集 2.0 为训练集，试基于 BIC 准则构建一个贝叶斯网.
**答：** 
关于贝叶斯网结构的构建，书中p160只稍微提了一下，不过还是挺好理解的，
《A Tutorial on Learning With Bayesian Networks》11节给出了更详细的描述。比较简单是方法就是贪心法：

1. 初始化一个网络结构；
2. 使用E表示当前合法的改变一条边的操作集合，比如若两个节点已经有连接，那么合法操作可以删除或者逆转，如没有连接则可以增加一条边，当前必须是在保证不会形成回路的情况；
3. 从中选择一个使得BIC下降最大的一个作为实际操作；
4. 循环2,3直到BIC不再下降。
论文中也给出了其他算法。

有时间再补代码吧。

---

#### 7.10 以西瓜数据集 2.0 中属性"脐部"为隐变量，试基于 EM 算法构建一个贝叶斯网.
**答：**

待填坑。
